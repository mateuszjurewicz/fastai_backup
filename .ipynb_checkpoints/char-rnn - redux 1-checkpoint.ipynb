{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Char RNN - redux 1\n",
    "Let's work on some Nietzsche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "from theano.sandbox import cuda\n",
    "# cuda.use('gpu2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "from __future__ import division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed, Activation\n",
    "from numpy.random import choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Let's prepare. We'll be trying to predict the next character using RNN architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corupus length: 600901\n"
     ]
    }
   ],
   "source": [
    "path = get_file(\"nietzsche.txt\", origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n",
    "text = open(path).read().lower() # switching to lowercase will make things easier\n",
    "print(\"corupus length:\", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are thinkers who believe in the saints.\r\n",
      "\r\n",
      "\r\n",
      "144\r\n",
      "\r\n",
      "It stands to reason that this sketch of the saint, made upon the model\r\n",
      "of the whole species, can be confronted with many opposing sketches that\r\n",
      "would create a more agreeable impression. There are certain exceptions\r\n",
      "among the species who distinguish themselves either by especial\r\n",
      "gentleness or especial humanity, and perhaps by the strength of their\r\n",
      "own personality. Others are in the highest degree fascinating because\r\n",
      "certain of their delusions shed a particular glow over their whole\r\n",
      "being, as is the case with the founder of christianity who took himself\r\n",
      "for the only begotten son of God and hence felt himself sinless; so that\r\n",
      "through his imagination--that should not be too harshly judged since the\r\n",
      "whole of antiquity swarmed with sons of god--he attained the same goal,\r\n",
      "the sense of complete sinlessness, complete irresponsibility, that can\r\n",
      "now be attained by every individual through science.--In the same manner\r\n",
      "I have viewed the saints of India who occupy an intermediate station\r\n",
      "between the christian saints and the Greek philosophers and hence are\r\n",
      "not to be regarded as a pure type. Knowledge and science--as far as they\r\n",
      "existed--and superiority to the rest of mankind by logical discipline\r\n",
      "and training of the intellectual powers were insisted upon by the\r\n",
      "Buddhists as essential to sanctity, just as they were denounced by the\r\n",
      "christian world as the indications of sinfulness."
     ]
    }
   ],
   "source": [
    "!tail {path} -n25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 60\n"
     ]
    }
   ],
   "source": [
    "# ok, so at this stage text holds a string with the entire Nietzsche text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)+1 # classic\n",
    "print(\"total chars:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n !\"\\'(),-.0123456789:;=?[]_abcdefghijklmnopqrstuvwxyz\\x86\\xa4\\xa6\\xa9\\xab\\xc3'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we will want to get rid of the \\x86 and the like? Apparently only for show..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars.insert(0, \"\\0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00\\n !\"\\'(),-.0123456789:;=?[]_abcdefghijklmnopqrstuvwxyz\\x86\\xa4\\xa6\\xa9\\xab\\xc3'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# construct backward and forward dictionaries\n",
    "char_indices = {c:i for i, c in enumerate(chars)}\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[43, 45, 32, 33, 28, 30, 32, 1, 1, 1]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now let's turn the entire text into a list of character indices\n",
    "idx = [char_indices[c] for c in text]\n",
    "idx[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'preface\\n\\n\\nsupposing that truth is a woman--what th'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see if it works\n",
    "tmp = []\n",
    "for i in idx[:50]:\n",
    "    tmp.append(indices_char[i])\n",
    "''.join(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# it works\n",
    "del tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess and Create Model\n",
    "Ok, so now we have our original text in numeric form that we can actually, possibly feed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 600862\n"
     ]
    }
   ],
   "source": [
    "# ok, so idx holds the entire text as a list of indices (that can be translated to chars)\n",
    "maxlen = 40  # this is the length of sentences we'll be considering (really artificial here)\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(idx) - maxlen + 1):\n",
    "    # ok, artificially separate the idx into \"sentences\" (I think these will be our training examples)\n",
    "    sentences.append(idx[i: i + maxlen])\n",
    "    \n",
    "    # and for every char in that \"sentence\" we can also store the next char (after the current one, including\n",
    "    # one that doesn't fit in the sentence - the one immediately following)\n",
    "    next_chars.append(idx[i+1: i+maxlen+1])\n",
    "    \n",
    "print(\"nb sequences:\", len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[43, 45, 32, 33, 28, 30, 32, 1, 1, 1]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[45, 32, 33, 28, 30, 32, 1, 1, 1, 46]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_chars[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ok, so now we're going to again mix the chars together? And turn them into np.ndarrays?\n",
    "sentences = np.concatenate([[np.array(o)] for o in sentences[:-2]])\n",
    "next_chars = np.concatenate([[np.array(o)] for o in next_chars[:-2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[43, 45, 32, 33, 28, 30, 32,  1,  1,  1, 46, 48, 43, 43, 42, 46, 36, 41, 34,  2, 47, 35, 28,\n",
       "        47,  2, 47, 45, 48, 47, 35,  2, 36, 46,  2, 28,  2, 50, 42, 40, 28],\n",
       "       [45, 32, 33, 28, 30, 32,  1,  1,  1, 46, 48, 43, 43, 42, 46, 36, 41, 34,  2, 47, 35, 28, 47,\n",
       "         2, 47, 45, 48, 47, 35,  2, 36, 46,  2, 28,  2, 50, 42, 40, 28, 41],\n",
       "       [32, 33, 28, 30, 32,  1,  1,  1, 46, 48, 43, 43, 42, 46, 36, 41, 34,  2, 47, 35, 28, 47,  2,\n",
       "        47, 45, 48, 47, 35,  2, 36, 46,  2, 28,  2, 50, 42, 40, 28, 41,  9],\n",
       "       [33, 28, 30, 32,  1,  1,  1, 46, 48, 43, 43, 42, 46, 36, 41, 34,  2, 47, 35, 28, 47,  2, 47,\n",
       "        45, 48, 47, 35,  2, 36, 46,  2, 28,  2, 50, 42, 40, 28, 41,  9,  9],\n",
       "       [28, 30, 32,  1,  1,  1, 46, 48, 43, 43, 42, 46, 36, 41, 34,  2, 47, 35, 28, 47,  2, 47, 45,\n",
       "        48, 47, 35,  2, 36, 46,  2, 28,  2, 50, 42, 40, 28, 41,  9,  9, 50]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:5]  # ok, so apparently they're still separated into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[45, 32, 33, 28, 30, 32,  1,  1,  1, 46, 48, 43, 43, 42, 46, 36, 41, 34,  2, 47, 35, 28, 47,\n",
       "         2, 47, 45, 48, 47, 35,  2, 36, 46,  2, 28,  2, 50, 42, 40, 28, 41],\n",
       "       [32, 33, 28, 30, 32,  1,  1,  1, 46, 48, 43, 43, 42, 46, 36, 41, 34,  2, 47, 35, 28, 47,  2,\n",
       "        47, 45, 48, 47, 35,  2, 36, 46,  2, 28,  2, 50, 42, 40, 28, 41,  9],\n",
       "       [33, 28, 30, 32,  1,  1,  1, 46, 48, 43, 43, 42, 46, 36, 41, 34,  2, 47, 35, 28, 47,  2, 47,\n",
       "        45, 48, 47, 35,  2, 36, 46,  2, 28,  2, 50, 42, 40, 28, 41,  9,  9],\n",
       "       [28, 30, 32,  1,  1,  1, 46, 48, 43, 43, 42, 46, 36, 41, 34,  2, 47, 35, 28, 47,  2, 47, 45,\n",
       "        48, 47, 35,  2, 36, 46,  2, 28,  2, 50, 42, 40, 28, 41,  9,  9, 50],\n",
       "       [30, 32,  1,  1,  1, 46, 48, 43, 43, 42, 46, 36, 41, 34,  2, 47, 35, 28, 47,  2, 47, 45, 48,\n",
       "        47, 35,  2, 36, 46,  2, 28,  2, 50, 42, 40, 28, 41,  9,  9, 50, 35]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I think we achieve this by wrapping it in another set of [] (we create an intermediate list)\n",
    "next_chars[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((600860, 40), (600860, 40))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.shape, next_chars.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's also establish the total number of latent factors we're looking for in a Nietzschean char :)))\n",
    "n_fac = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, LSTM, Dropout, TimeDistributed, Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model=Sequential([\n",
    "        Embedding(vocab_size, n_fac, input_length=maxlen),\n",
    "        LSTM(512, input_dim=n_fac,return_sequences=True, dropout_U=0.2, dropout_W=0.2,\n",
    "             consume_less='gpu'),\n",
    "        Dropout(0.2),\n",
    "        LSTM(512, return_sequences=True, dropout_U=0.2, dropout_W=0.2,\n",
    "             consume_less='gpu'),\n",
    "        Dropout(0.2),\n",
    "        TimeDistributed(Dense(vocab_size)),\n",
    "        Activation('softmax')\n",
    "    ])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sparse categorical crossentropy expects one definitve class (one-hot encoded 1, in a sea of zeros)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=Adam())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit (and show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[45],\n",
       "        [32],\n",
       "        [33],\n",
       "        ..., \n",
       "        [40],\n",
       "        [28],\n",
       "        [41]],\n",
       "\n",
       "       [[32],\n",
       "        [33],\n",
       "        [28],\n",
       "        ..., \n",
       "        [28],\n",
       "        [41],\n",
       "        [ 9]],\n",
       "\n",
       "       [[33],\n",
       "        [28],\n",
       "        [30],\n",
       "        ..., \n",
       "        [41],\n",
       "        [ 9],\n",
       "        [ 9]],\n",
       "\n",
       "       ..., \n",
       "       [[36],\n",
       "        [28],\n",
       "        [41],\n",
       "        ..., \n",
       "        [39],\n",
       "        [41],\n",
       "        [32]],\n",
       "\n",
       "       [[28],\n",
       "        [41],\n",
       "        [ 2],\n",
       "        ..., \n",
       "        [41],\n",
       "        [32],\n",
       "        [46]],\n",
       "\n",
       "       [[41],\n",
       "        [ 2],\n",
       "        [50],\n",
       "        ..., \n",
       "        [32],\n",
       "        [46],\n",
       "        [46]]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ok, so this will contain arrays of next chars? That's the y...\n",
    "np.expand_dims(next_chars, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(sentences, np.expand_dims(next_chars,-1), batch_size=64, nb_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_example():\n",
    "    seed_string=\"ethics is a basic foundation of all that\"\n",
    "    for i in range(320):\n",
    "        x=np.array([char_indices[c] for c in seed_string[-40:]])[np.newaxis,:]\n",
    "        preds = model.predict(x, verbose=0)[0][-1]\n",
    "        preds = preds/np.sum(preds)\n",
    "        next_char = choice(chars, p=preds)\n",
    "        seed_string = seed_string + next_char\n",
    "    print(seed_string)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
