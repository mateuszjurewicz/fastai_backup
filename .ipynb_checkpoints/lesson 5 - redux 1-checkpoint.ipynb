{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5 - NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "from theano.sandbox import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "from __future__ import division, print_function\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = \"kaggle_data/imdb/models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imdb sentiment analysis dset is built into keras (like mnist)\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it's just a dict, words are keys, index is the value\n",
    "# BUT - they have also been index by frequency of occurrence, so idx nr 3 is the 3rd most common word\n",
    "type(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88584"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'and', 'a', 'of', 'to', 'is', 'br', 'in', 'it', 'i']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can obtain the list of words from the idx\n",
    "# we're using the get() method which will take a key and lookup the value of that key in the idx. So again, frequency.\n",
    "idx_arr = sorted(idx, key=idx.get)\n",
    "idx_arr[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's also get the mapping from idx to word\n",
    "idx2word =  {i:w for w, i in idx.items()}\n",
    "idx3word = {v: k for k, v in idx.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word == idx3word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load up the actual data\n",
    "path = get_file('imdb_full.pkl',\n",
    "                origin='https://s3.amazonaws.com/text-datasets/imdb_full.pkl',\n",
    "                md5_hash='d091312047c43cf9e4e38fef92437263')\n",
    "f = open(path, 'rb')\n",
    "(x_train, labels_train), (x_test, labels_test) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23022, 309, 6, 3, 1069, 209, 9, 2175, 30, 1]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's grab the first review (the one about bromwell high) and decode it to words\n",
    "x_train[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my 35 years in the teaching profession lead me to believe that bromwell high's satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers' pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled at high a classic line inspector i'm here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it isn't\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_in_words = []\n",
    "for i in x_train[0]:\n",
    "    review_in_words.append(idx2word[i])\n",
    "    \n",
    "review_in_words = \" \".join(review_in_words)\n",
    "review_in_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look at the labels (we're looking at 2 categories - positive and negative reviews)\n",
    "labels_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from what I see we might wanna shuffle before training.\n",
    "labels_train[-10::]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduce dimensionality by turning less frequent word tokens into the same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "trn = [np.array([word_idx if word_idx <vocab_size - 1 else vocab_size - 1 for word_idx in review]) for review in x_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4999,  309,    6,    3, 1069,  209,    9, 2175,   30,    1,  169,   55,   14,   46,   82,\n",
       "       4999,   41,  393,  110,  138,   14, 4999,   58, 4477,  150,    8,    1, 4999, 4999,  482,\n",
       "         69,    5,  261,   12, 4999, 4999, 2003,    6,   73, 2436,    5,  632,   71,    6, 4999,\n",
       "          1, 4999,    5, 2004, 4999,    1, 4999, 1534,   34,   67,   64,  205,  140,   65, 1232,\n",
       "       4999, 4999,    1, 4999,    4,    1,  223,  901,   29, 3024,   69,    4,    1, 4999,   10,\n",
       "        694,    2,   65, 1534,   51,   10,  216,    1,  387,    8,   60,    3, 1472, 3724,  802,\n",
       "          5, 3521,  177,    1,  393,   10, 1238, 4999,   30,  309,    3,  353,  344, 2989,  143,\n",
       "        130,    5, 4999,   28,    4,  126, 4999, 1472, 2375,    5, 4999,  309,   10,  532,   12,\n",
       "        108, 1470,    4,   58,  556,  101,   12, 4999,  309,    6,  227, 4187,   48,    3, 2237,\n",
       "         12,    9,  215])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can see that all of the less frequent words (with higher indices) became 4999.\n",
    "# I think that's because there's also a zeroth word\n",
    "trn[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  10,  432,    2,  216,   11,   17,  233,  311,  100,  109, 4999,    5,   31,    3,  168,\n",
       "        366,    4, 1920,  634,  971,   12,   10,   13, 4999,    5,   64,    9,   85,   36,   48,\n",
       "         10,  694,    4, 4999, 4999,   26,   13,   61,  499,    5,   78,  209,   10,   13,  352,\n",
       "       4999,  253,    1,  106,    4, 3270, 4999,   52,   70,    2, 1839, 4999,  253, 1019, 4999,\n",
       "         16,  138, 4999,    1, 1910,    4,    3,   49,   17,    6,   12,    9,   67, 2885,   16,\n",
       "        260, 1435,   11,   28,  119,  615,   12,    1,  433,  747,   60,   13, 2959,   43,   13,\n",
       "       3080,   31, 2126,  312,    1,   83,  317,    4,    1,   17,    2,   68, 1678,    5, 1671,\n",
       "        312,    1,  330,  317,  134, 4999,    1,  747,   10,   21,   61,  216,  108,  369,    8,\n",
       "       1671,   18,  108,  365, 2068,  346,   14,   70,  266, 2721,   21,    5,  384,  256,   64,\n",
       "         95, 2575,   11,   17,   13,   84,    2,   10, 1464,   12,   22,  137,   64,    9,  156,\n",
       "         22, 1916])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repeat for test\n",
    "test = [np.array([wi if wi < vocab_size -1 else vocab_size - 1 for wi in r])for r in x_test]\n",
    "test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show length of reviews\n",
    "To figure out what the max, min and average is, so that we can again reduce some dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we can use the map function to pass all elements of trn to the len() function and get an array of lenghts\n",
    "lens = np.array(map(len, trn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2493, 10, 237.71364)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(lens), min(lens), lens.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Seems that 500 words per review might be enough (given a mean of 240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# there's a keras preprocessing function that helps turn vectors to a specific lenght (trimming and padding with\n",
    "# a pre-specified padding char/number\n",
    "from keras.preprocessing import sequence\n",
    "pad_length = 500\n",
    "pad_thingy = 0\n",
    "\n",
    "# padding with 0 is tricky, it's \"the\", the most common word token. So we assume it will carry the least meaning\n",
    "trn = sequence.pad_sequences(trn, maxlen=pad_length, value=pad_thingy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0, 4999,  309,    6,    3, 1069,  209,    9, 2175,   30,    1,  169,   55,   14,\n",
       "         46,   82, 4999,   41,  393,  110,  138,   14, 4999,   58, 4477,  150,    8,    1, 4999,\n",
       "       4999,  482,   69,    5,  261,   12, 4999, 4999, 2003,    6,   73, 2436,    5,  632,   71,\n",
       "          6, 4999,    1, 4999,    5, 2004, 4999,    1, 4999, 1534,   34,   67,   64,  205,  140,\n",
       "         65, 1232, 4999, 4999,    1, 4999,    4,    1,  223,  901,   29, 3024,   69,    4,    1,\n",
       "       4999,   10,  694,    2,   65, 1534,   51,   10,  216,    1,  387,    8,   60,    3, 1472,\n",
       "       3724,  802,    5, 3521,  177,    1,  393,   10, 1238, 4999,   30,  309,    3,  353,  344,\n",
       "       2989,  143,  130,    5, 4999,   28,    4,  126, 4999, 1472, 2375,    5, 4999,  309,   10,\n",
       "        532,   12,  108, 1470,    4,   58,  556,  101,   12, 4999,  309,    6,  227, 4187,   48,\n",
       "          3, 2237,   12,    9,  215], dtype=int32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# by default it pre-pads\n",
    "trn[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,   10,  432,    2,  216,   11,   17,  233,  311,  100,  109, 4999,    5,\n",
       "         31,    3,  168,  366,    4, 1920,  634,  971,   12,   10,   13, 4999,    5,   64,    9,\n",
       "         85,   36,   48,   10,  694,    4, 4999, 4999,   26,   13,   61,  499,    5,   78,  209,\n",
       "         10,   13,  352, 4999,  253,    1,  106,    4, 3270, 4999,   52,   70,    2, 1839, 4999,\n",
       "        253, 1019, 4999,   16,  138, 4999,    1, 1910,    4,    3,   49,   17,    6,   12,    9,\n",
       "         67, 2885,   16,  260, 1435,   11,   28,  119,  615,   12,    1,  433,  747,   60,   13,\n",
       "       2959,   43,   13, 3080,   31, 2126,  312,    1,   83,  317,    4,    1,   17,    2,   68,\n",
       "       1678,    5, 1671,  312,    1,  330,  317,  134, 4999,    1,  747,   10,   21,   61,  216,\n",
       "        108,  369,    8, 1671,   18,  108,  365, 2068,  346,   14,   70,  266, 2721,   21,    5,\n",
       "        384,  256,   64,   95, 2575,   11,   17,   13,   84,    2,   10, 1464,   12,   22,  137,\n",
       "         64,    9,  156,   22, 1916], dtype=int32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repeat for test\n",
    "test = sequence.pad_sequences(test, maxlen=pad_length, value=pad_thingy)\n",
    "test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 500)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create simple models\n",
    "We'll start with a single hidden layer model, for a benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Hidden NN\n",
    "Note that we can't expect much by just feeding word indexes into the NN, we have to use the embedding, otherwise the would be worth zero and bromwell would be worth 4999."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from keras.layers import Dense, Dropout, Convolution1D, MaxPooling1D, Embedding, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the input to the embedding layer is the number of word indices (nr of our vocab - they'll get looked up by idx)\n",
    "# the output is the number of latent factors (dimension we think describe each word)\n",
    "# and finally we need to tell it how long each sentence is (padded it to 500)\n",
    "model = Sequential([\n",
    "        Embedding(vocab_size, 32, input_length=pad_length),\n",
    "        Flatten(), # gotta flatten cause now we have 500 words * 32 embeddings (matrix, not vector)\n",
    "        Dense(100, activation=\"relu\"),\n",
    "        Dropout(.7),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=Adam(), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_2 (Embedding)          (None, 500, 32)       160000      embedding_input_2[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 16000)         0           embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 100)           1600100     flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 100)           0           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 1)             101         dropout_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 1760201\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 8s - loss: 0.4369 - acc: 0.7724 - val_loss: 0.2887 - val_acc: 0.8768\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 8s - loss: 0.1857 - acc: 0.9306 - val_loss: 0.3450 - val_acc: 0.8574\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 8s - loss: 0.0737 - acc: 0.9751 - val_loss: 0.4125 - val_acc: 0.8600\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 8s - loss: 0.0265 - acc: 0.9920 - val_loss: 0.5732 - val_acc: 0.8559\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 8s - loss: 0.0151 - acc: 0.9952 - val_loss: 0.6711 - val_acc: 0.8546\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe3ebee1550>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, labels_train, batch_size=32, nb_epoch=5, \n",
    "          validation_data=(test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv + maxpooling\n",
    "Is likely to work better since we have ordered data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        Embedding(vocab_size, 32, input_length=pad_length),\n",
    "        # Flatten() shouldn't be used because we're going to use convolution (like an image, flatten after all convo\n",
    "        Convolution1D(64, 5, border_mode=\"same\", activation=\"relu\"), # we have to set the filter nr and size (65, 5)\n",
    "        MaxPooling1D(),\n",
    "        Flatten(),\n",
    "        Dense(100, activation=\"relu\"),\n",
    "        Dropout(.7),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=Adam(), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 18s - loss: 0.4153 - acc: 0.7866 - val_loss: 0.2608 - val_acc: 0.8908\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 17s - loss: 0.2348 - acc: 0.9127 - val_loss: 0.2700 - val_acc: 0.8872\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 17s - loss: 0.1801 - acc: 0.9344 - val_loss: 0.2883 - val_acc: 0.8892\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 17s - loss: 0.1356 - acc: 0.9524 - val_loss: 0.3374 - val_acc: 0.8866\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 17s - loss: 0.0992 - acc: 0.9663 - val_loss: 0.3736 - val_acc: 0.8820\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe3dae93710>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, labels_train, validation_data=(test, labels_test), batch_size=32, nb_epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we can adjust and use dropout within the embedding too (effectively dropping certain words)\n",
    "conv2 = Sequential([\n",
    "        Embedding(vocab_size, 32, input_length=pad_length, dropout=0.2), # adding dropout within the embedding\n",
    "        Dropout(.2),\n",
    "        Convolution1D(64, 5, border_mode=\"same\", activation=\"relu\"),\n",
    "        Dropout(.2),\n",
    "        MaxPooling1D(),\n",
    "        Flatten(),\n",
    "        Dense(100, activation=\"relu\"),\n",
    "        Dropout(.7),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv2.compile(loss=\"binary_crossentropy\", optimizer=Adam(), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 15s - loss: 0.5328 - acc: 0.6953 - val_loss: 0.3086 - val_acc: 0.8754\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 15s - loss: 0.3088 - acc: 0.8794 - val_loss: 0.2570 - val_acc: 0.8950\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 15s - loss: 0.2665 - acc: 0.8963 - val_loss: 0.2620 - val_acc: 0.8913\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 15s - loss: 0.2460 - acc: 0.9056 - val_loss: 0.2537 - val_acc: 0.8964\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 15s - loss: 0.2281 - acc: 0.9127 - val_loss: 0.2563 - val_acc: 0.8955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe3d1712b50>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2.fit(trn, labels_train, validation_data=(test, labels_test), batch_size=64, nb_epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv2.save_weights(model_path + \"conv2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv2.load_weights(model_path + \"conv2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# zippity doo-dah, we've beaten academia\n",
    "# now we'll try to use the glove embeddings instead of training our own"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained Glove Embeddings\n",
    "Let's use those and see what happizens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_vectors(loc):\n",
    "    return (load_array(loc+'.dat'),\n",
    "        pickle.load(open(loc+'_words.pkl','rb')),\n",
    "        pickle.load(open(loc+'_idx.pkl','rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glove_path = 'kaggle_data/glove/results/'\n",
    "vecs, words, wordidx = load_vectors(glove_path + '6B.50d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', ',', '.', 'of', 'to', 'and', 'in', 'a', '\"', \"'s\"]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so we've got the actual vectors in vecs, the words in words and a matching dictionary in wordidx\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 50)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we want to figure out how to remap our words and indexes into Glove's, and handle cases where words have no match\n",
    "vecs.shape[0], vecs.shape[1] # 50 dimensions per word in Glove\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_emb():\n",
    "    # this grabs the number of dimensions per word in Glove (50)\n",
    "    n_fact = vecs.shape[1]\n",
    "    \n",
    "    # this creates a zero-filled matrix of 4999 by 50 (so each of the words in our vocabulary will get turned to \n",
    "    # 50 dimensional vector from Glove\n",
    "    emb = np.zeros((vocab_size, n_fact))\n",
    "\n",
    "    for i in range(1,len(emb)):\n",
    "        # here we grab each of our current words (5000)\n",
    "        word = idx2word[i]\n",
    "        if word and re.match(r\"^[a-zA-Z0-9\\-]*$\", word):\n",
    "            # here we grab the index in Glove of that word\n",
    "            src_idx = wordidx[word]\n",
    "            # and we put the corresponding vector in our embedding matrix, at the index of that word\n",
    "            emb[i] = vecs[src_idx]\n",
    "        else:\n",
    "            # If we can't find the word in glove, randomly initialize\n",
    "            emb[i] = normal(scale=0.6, size=(n_fact,))\n",
    "\n",
    "    # This is our \"rare word\" id - we want to randomly initialize\n",
    "    emb[-1] = normal(scale=0.6, size=(n_fact,))\n",
    "    emb/=3\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb = create_emb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 50)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0406, -0.0468,  0.0542, -0.0995,  0.1925, -0.177 ,  0.1945,  0.4075,  0.2944,  0.04  ,\n",
       "        0.2115, -0.0569, -0.0217,  0.3187, -0.2585,  0.0222,  0.4323, -0.2497, -0.0955, -0.1574,\n",
       "        0.0973,  0.0809, -0.1142, -0.0458,  0.0273, -0.1005, -0.1229, -0.1304,  0.2392, -0.0945,\n",
       "       -0.2137, -0.0172, -0.3156,  0.2925, -0.0208,  0.259 , -0.0255, -0.0654, -0.1538,  0.1738,\n",
       "        0.0362, -0.113 , -0.1628, -0.0823, -0.0871,  0.2895,  0.1051,  0.1764,  0.01  ,  0.5174])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[4999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# notice that our 0 (the) becomes meaningless because its vector gets filled with 0\n",
    "emb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we can very easily pass our embedding matrix to the keras.layers.Embedding constructor (weights parameter)\n",
    "\n",
    "glove_model = Sequential([\n",
    "        Embedding(vocab_size, 50, input_length=pad_length, weights=[emb], dropout=0.2, trainable=False),\n",
    "        # since we want to keep these embeddings we set them to be non-trainable\n",
    "        Dropout(.25),\n",
    "        Convolution1D(64, 5, activation=\"relu\"),\n",
    "        Dropout(.25),\n",
    "        MaxPooling1D(),\n",
    "        Flatten(),\n",
    "        Dense(100, activation=\"relu\"),\n",
    "        Dropout(.7),\n",
    "        Dense(1, activation=\"sigmoid\"),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_model.compile(loss=\"binary_crossentropy\", optimizer=Adam(), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 16s - loss: 0.6161 - acc: 0.6492 - val_loss: 0.5019 - val_acc: 0.7729\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 16s - loss: 0.5154 - acc: 0.7526 - val_loss: 0.4689 - val_acc: 0.8019\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 16s - loss: 0.4851 - acc: 0.7741 - val_loss: 0.4411 - val_acc: 0.8177\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 16s - loss: 0.4672 - acc: 0.7838 - val_loss: 0.4451 - val_acc: 0.8116\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 16s - loss: 0.4506 - acc: 0.7931 - val_loss: 0.4164 - val_acc: 0.8256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe3be7e6150>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model.fit(trn, labels_train, validation_data=(test, labels_test), batch_size=64, nb_epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we can also try to finetune the original glove weights - especially since words that weren't in the set\n",
    "# of the Glove tokens get random weights.\n",
    "# we just needs to reset the first (Embedding) layer to trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 16s - loss: 0.3923 - acc: 0.8237 - val_loss: 0.3998 - val_acc: 0.8226\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 16s - loss: 0.3879 - acc: 0.8270 - val_loss: 0.4004 - val_acc: 0.8206\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 16s - loss: 0.3803 - acc: 0.8287 - val_loss: 0.4199 - val_acc: 0.8030\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 16s - loss: 0.3811 - acc: 0.8327 - val_loss: 0.4212 - val_acc: 0.8015\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 16s - loss: 0.3702 - acc: 0.8358 - val_loss: 0.3769 - val_acc: 0.8367\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe3bee25210>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].trainable = True\n",
    "model.optimizer.lr = 1e-4\n",
    "glove_model.fit(trn, labels_train, validation_data=(test, labels_test), batch_size=64, nb_epoch=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-size CNN\n",
    "From the thing Ben Bowles did in Quid. The idea here is to use the Functional, instead of Sequential model and then add the results from different-sized convolutional layers and concatenate them and pass as vectors to the Dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we're still using our Glove embeddings as input\n",
    "graph_in = Input((vocab_size, 50))\n",
    "convs = []\n",
    "\n",
    "# we'll try 3, 4 and 5 as filter sizes\n",
    "for fsz in range(3, 6):\n",
    "    x = Convolution1D(64, fsz, border_mode=\"same\", activation=\"relu\")(graph_in)\n",
    "    x = MaxPooling1D()(x)\n",
    "    x = Flatten()(x)\n",
    "    convs.append(x)\n",
    "\n",
    "out = Merge(mode=\"concat\")(convs)\n",
    "graph = Model(graph_in, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb = create_emb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# replace the conv/maxpool layers in our original model with the multi-size convs\n",
    "multi_conv_model = Sequential([\n",
    "        Embedding(vocab_size, 50, input_length=pad_length, weights=[emb]),\n",
    "        Dropout(.2),\n",
    "        graph,\n",
    "        Dropout(.5),\n",
    "        Dense(100, activation=\"relu\"),\n",
    "        Dropout(.7),\n",
    "        Dense(1, activation=\"sigmoid\"),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "multi_conv_model.compile(loss=\"binary_crossentropy\", optimizer=Adam(), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 40s - loss: 0.5077 - acc: 0.7328 - val_loss: 0.2817 - val_acc: 0.8832\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 39s - loss: 0.2801 - acc: 0.8888 - val_loss: 0.2606 - val_acc: 0.8924\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 39s - loss: 0.2279 - acc: 0.9114 - val_loss: 0.2721 - val_acc: 0.8887\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 39s - loss: 0.1963 - acc: 0.9247 - val_loss: 0.2587 - val_acc: 0.8952\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 39s - loss: 0.1750 - acc: 0.9335 - val_loss: 0.2669 - val_acc: 0.8940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe3ba72c310>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_conv_model.fit(trn, labels_train, validation_data=(test, labels_test), nb_epoch=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "Beyon the beyond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_11 (Embedding)         (None, 500, 32)       160000      embedding_input_10[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 100)           53200       embedding_11[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_20 (Dense)                 (None, 1)             101         lstm_1[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 213301\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Embedding(vocab_size, 32, input_length=pad_lengthngth, mask_zero=True,\n",
    "              W_regularizer=l2(1e-6), dropout=0.2),\n",
    "    LSTM(100, consume_less='gpu'),\n",
    "    Dense(1, activation='sigmoid')])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 188s - loss: 0.5632 - acc: 0.7257 - val_loss: 0.4168 - val_acc: 0.8253\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 188s - loss: 0.4708 - acc: 0.7832 - val_loss: 0.4265 - val_acc: 0.8206\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe3a988c910>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, labels_train, validation_data=(test, labels_test), nb_epoch=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
